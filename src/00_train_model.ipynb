{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from tqdm import tqdm\n",
    "import wandb\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from models.transformer import Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NpyDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, X_path, Y_path):\n",
    "        self.x = np.load(X_path, mmap_mode='r')\n",
    "        self.y = np.load(Y_path, mmap_mode='r')\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.x.shape[0]\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        x = self.x[idx]\n",
    "        y = self.y[idx]\n",
    "        return x.astype(int), y.astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<bound method Module.parameters of Transformer(\n",
      "  (embed): Embedding(27038, 16)\n",
      "  (attn_ops): ModuleList(\n",
      "    (0-3): 4 x SelfAttention(\n",
      "      (query): Linear(in_features=16, out_features=64, bias=True)\n",
      "      (key): Linear(in_features=16, out_features=64, bias=True)\n",
      "      (value): Linear(in_features=16, out_features=64, bias=True)\n",
      "      (softmax): Softmax(dim=2)\n",
      "      (output): Linear(in_features=64, out_features=16, bias=True)\n",
      "    )\n",
      "  )\n",
      "  (first_layernorms): ModuleList(\n",
      "    (0-3): 4 x LayerNorm((16,), eps=1e-05, elementwise_affine=True)\n",
      "  )\n",
      "  (feed_fwd_layers): ModuleList(\n",
      "    (0-3): 4 x Linear(in_features=16, out_features=16, bias=True)\n",
      "  )\n",
      "  (feed_fwd_activations): ModuleList(\n",
      "    (0-3): 4 x Sigmoid()\n",
      "  )\n",
      "  (second_layernorms): ModuleList(\n",
      "    (0-3): 4 x LayerNorm((16,), eps=1e-05, elementwise_affine=True)\n",
      "  )\n",
      "  (final_mlp): MLP(\n",
      "    (feed_fwd): Linear(in_features=16, out_features=16, bias=True)\n",
      "    (sigmoid): Sigmoid()\n",
      "    (feed_fwd_2): Linear(in_features=16, out_features=47, bias=True)\n",
      "  )\n",
      ")>\n"
     ]
    }
   ],
   "source": [
    "batch_size=16428\n",
    "train_dataset = NpyDataset('../data/Xtrain_base.npy', '../data/Ytrain_base.npy')\n",
    "train_dataloader = torch.utils.data.DataLoader(train_dataset,batch_size=batch_size,shuffle=True)\n",
    "test_dataset = NpyDataset('../data/Xtest_base.npy', '../data/Ytest_base.npy')\n",
    "test_dataloader = torch.utils.data.DataLoader(test_dataset,batch_size=batch_size,shuffle=True)\n",
    "\n",
    "vocab = pd.read_csv('../data/vocab.csv', header=None, index_col=0)\n",
    "vocab_size = vocab.shape[0]\n",
    "cat_label_mapping = pd.read_csv('../data/cat_label_mapping.csv', header=None)\n",
    "num_classes = cat_label_mapping.shape[0]\n",
    "context_length = train_dataset.x.shape[1]\n",
    "\n",
    "\n",
    "device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "net = Transformer(num_layers=4, vocab_size=vocab_size, num_tokens=context_length, d_model=16, d_q_k_v=16, num_heads=4, num_classes=num_classes, hidden_dim=16)\n",
    "net.to(device=device)\n",
    "optim = torch.optim.AdamW(params = net.parameters())\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "epochs=100\n",
    "\n",
    "\n",
    "print(net.parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.int64(452239)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sum([param.numel() for param in net.parameters()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing training loss\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([7000, 2049, 4, 16]) torch.Size([7000, 2049, 4, 16])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Invalid buffer size: 437.93 GB",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mRuntimeError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 11\u001b[39m\n\u001b[32m      9\u001b[39m batch_x = batch_x.to(device=device).to(torch.long)\n\u001b[32m     10\u001b[39m batch_y = batch_y.to(device=device)\n\u001b[32m---> \u001b[39m\u001b[32m11\u001b[39m pred_batch = \u001b[43mnet\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch_x\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     12\u001b[39m loss = loss_fn(pred_batch,batch_y).item()\n\u001b[32m     13\u001b[39m lossTrain+=loss\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniforge3/envs/cell-type-classification/lib/python3.13/site-packages/torch/nn/modules/module.py:1739\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1737\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1738\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1739\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniforge3/envs/cell-type-classification/lib/python3.13/site-packages/torch/nn/modules/module.py:1750\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1745\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1746\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1747\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1748\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1749\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1750\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1752\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1753\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/git_repos/cell-type-classification/src/models/transformer.py:72\u001b[39m, in \u001b[36mTransformer.forward\u001b[39m\u001b[34m(self, input)\u001b[39m\n\u001b[32m     61\u001b[39m \u001b[38;5;66;03m# # print(f\"Embedding {embedding.shape}\")\u001b[39;00m\n\u001b[32m     63\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m (attn_op,\n\u001b[32m     64\u001b[39m      first_layernorm,\n\u001b[32m     65\u001b[39m      feed_fwd_layer,\n\u001b[32m   (...)\u001b[39m\u001b[32m     70\u001b[39m                               \u001b[38;5;28mself\u001b[39m.feed_fwd_activations,\n\u001b[32m     71\u001b[39m                               \u001b[38;5;28mself\u001b[39m.second_layernorms):\n\u001b[32m---> \u001b[39m\u001b[32m72\u001b[39m     attn_output = \u001b[43mattn_op\u001b[49m\u001b[43m(\u001b[49m\u001b[43membedding\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     73\u001b[39m     \u001b[38;5;66;03m# print(f\"Attention output {attn_output.shape}\")\u001b[39;00m\n\u001b[32m     74\u001b[39m     residual_added = embedding + attn_output\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniforge3/envs/cell-type-classification/lib/python3.13/site-packages/torch/nn/modules/module.py:1739\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1737\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1738\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1739\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniforge3/envs/cell-type-classification/lib/python3.13/site-packages/torch/nn/modules/module.py:1750\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1745\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1746\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1747\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1748\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1749\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1750\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1752\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1753\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/git_repos/cell-type-classification/src/models/self_attention.py:57\u001b[39m, in \u001b[36mSelfAttention.forward\u001b[39m\u001b[34m(self, embedding)\u001b[39m\n\u001b[32m     51\u001b[39m values = torch.reshape(\n\u001b[32m     52\u001b[39m     \u001b[38;5;28mself\u001b[39m.value(embedding),\n\u001b[32m     53\u001b[39m     (batch_size, \u001b[38;5;28mself\u001b[39m.num_tokens, \u001b[38;5;28mself\u001b[39m.num_heads, \u001b[38;5;28mself\u001b[39m.d_q_k_v),\n\u001b[32m     54\u001b[39m )\n\u001b[32m     56\u001b[39m \u001b[38;5;28mprint\u001b[39m(queries.shape, keys.shape)\n\u001b[32m---> \u001b[39m\u001b[32m57\u001b[39m token_weights_pre_softmax = \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43meinsum\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mbthd, buhd->btuh\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mqueries\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkeys\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     58\u001b[39m token_weights_pre_softmax_scaled = torch.div(\n\u001b[32m     59\u001b[39m     token_weights_pre_softmax, torch.sqrt(torch.tensor(\u001b[38;5;28mself\u001b[39m.d_model))\n\u001b[32m     60\u001b[39m )\n\u001b[32m     61\u001b[39m softmaxed_weights = \u001b[38;5;28mself\u001b[39m.softmax(token_weights_pre_softmax_scaled)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniforge3/envs/cell-type-classification/lib/python3.13/site-packages/torch/functional.py:407\u001b[39m, in \u001b[36meinsum\u001b[39m\u001b[34m(*args)\u001b[39m\n\u001b[32m    402\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m einsum(equation, *_operands)\n\u001b[32m    404\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(operands) <= \u001b[32m2\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m opt_einsum.enabled:\n\u001b[32m    405\u001b[39m     \u001b[38;5;66;03m# the path for contracting 0 or 1 time(s) is already optimized\u001b[39;00m\n\u001b[32m    406\u001b[39m     \u001b[38;5;66;03m# or the user has disabled using opt_einsum\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m407\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_VF\u001b[49m\u001b[43m.\u001b[49m\u001b[43meinsum\u001b[49m\u001b[43m(\u001b[49m\u001b[43mequation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moperands\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# type: ignore[attr-defined]\u001b[39;00m\n\u001b[32m    409\u001b[39m path = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    410\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m opt_einsum.is_available():\n",
      "\u001b[31mRuntimeError\u001b[39m: Invalid buffer size: 437.93 GB"
     ]
    }
   ],
   "source": [
    "# wandb.init(project=\"cell_type_classification\",config={})\n",
    "\n",
    "for e in range(epochs):\n",
    "    net.eval()\n",
    "    print(\"Computing training loss\")\n",
    "    lossTrain = 0.0\n",
    "    num_batches = 0\n",
    "    for batch_x,batch_y in tqdm(train_dataloader):\n",
    "        batch_x = batch_x.to(device=device).to(torch.long)\n",
    "        batch_y = batch_y.to(device=device)\n",
    "        pred_batch = net(batch_x)\n",
    "        loss = loss_fn(pred_batch,batch_y).item()\n",
    "        lossTrain+=loss\n",
    "        num_batches+=1\n",
    "        loss_fn.zero_grad()\n",
    "\n",
    "    lossTrain/=num_batches\n",
    "\n",
    "    print(\"Computing testing loss\")\n",
    "    lossTest = 0.0\n",
    "    num_batches = 0\n",
    "    for batch_x,batch_y in tqdm(test_dataloader):\n",
    "        batch_x = batch_x.to(device=device).to(torch.long)\n",
    "        batch_y = batch_y.to(device=device)\n",
    "        pred_batch = net(batch_x)\n",
    "        loss = loss_fn(pred_batch,batch_y).item()\n",
    "        lossTest+=loss\n",
    "        num_batches+=1\n",
    "        loss_fn.zero_grad()\n",
    "    \n",
    "    lossTest/=num_batches\n",
    "\n",
    "    print({\"train_loss\": lossTrain, \"test_loss\": lossTest})\n",
    "    # wandb.log({\"train_loss\": lossTrain, \"test_loss\": lossTest})\n",
    "\n",
    "    print(\"Training model\")\n",
    "    net.train()\n",
    "    for i, (batch_x, batch_y) in tqdm(enumerate(train_dataloader)):\n",
    "        batch_x = batch_x.to(device=device).to(torch.long)\n",
    "        batch_y = batch_y.to(device=device)\n",
    "        pred = net(batch_x)\n",
    "        loss = loss_fn(pred,batch_y)\n",
    "        loss.backward()\n",
    "        optim.step()\n",
    "        optim.zero_grad()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from models.self_attention import SelfAttention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "attn_block = nn.Sequential()\n",
    "attn_block.add_module('attn_1', SelfAttention().to(device='mps'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method Module.named_modules of Sequential(\n",
       "  (attn_1): SelfAttention(\n",
       "    (query): Linear(in_features=512, out_features=768, bias=True)\n",
       "    (key): Linear(in_features=512, out_features=768, bias=True)\n",
       "    (value): Linear(in_features=512, out_features=768, bias=True)\n",
       "    (softmax): Softmax(dim=2)\n",
       "    (output): Linear(in_features=768, out_features=512, bias=True)\n",
       "  )\n",
       ")>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attn_block.named_modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "layer = attn_block.modules().__next__()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sequential(\n",
       "  (attn_1): SelfAttention(\n",
       "    (query): Linear(in_features=512, out_features=768, bias=True)\n",
       "    (key): Linear(in_features=512, out_features=768, bias=True)\n",
       "    (value): Linear(in_features=512, out_features=768, bias=True)\n",
       "    (softmax): Softmax(dim=2)\n",
       "    (output): Linear(in_features=768, out_features=512, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_layers = 4\n",
    "attn_blocks = nn.ModuleList()\n",
    "num_tokens = 2048\n",
    "d_model = 512\n",
    "d_q_k_v = 16\n",
    "num_heads = 4\n",
    "\n",
    "for i in range(num_layers):\n",
    "    attn_block = nn.Sequential()\n",
    "    attn_block.add_module(f'attn_{i}', SelfAttention(num_tokens, d_model, d_q_k_v, num_heads).to(device='mps'))\n",
    "    attn_block.add_module(f'ln1_{i}', nn.LayerNorm(d_model).to(device='mps'))\n",
    "    attn_block.add_module(f'fwd_{i}', nn.Linear(in_features=d_model, out_features=d_model).to(device='mps'))\n",
    "    attn_block.add_module(f'fwd_activation_{i}', nn.Sigmoid().to(device='mps'))\n",
    "    attn_block.add_module(f'ln2_{i}', nn.LayerNorm(d_model).to(device='mps'))\n",
    "    attn_blocks.add_module(f'attn_block_{i}', attn_block)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'str' object cannot be interpreted as an integer",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[47]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[43mattn_blocks\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mattn_0\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniforge3/envs/cell-type-classification/lib/python3.13/site-packages/torch/nn/modules/container.py:334\u001b[39m, in \u001b[36mModuleList.__getitem__\u001b[39m\u001b[34m(self, idx)\u001b[39m\n\u001b[32m    332\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.\u001b[34m__class__\u001b[39m(\u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mself\u001b[39m._modules.values())[idx])\n\u001b[32m    333\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m334\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._modules[\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_get_abs_string_index\u001b[49m\u001b[43m(\u001b[49m\u001b[43midx\u001b[49m\u001b[43m)\u001b[49m]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniforge3/envs/cell-type-classification/lib/python3.13/site-packages/torch/nn/modules/container.py:314\u001b[39m, in \u001b[36mModuleList._get_abs_string_index\u001b[39m\u001b[34m(self, idx)\u001b[39m\n\u001b[32m    312\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_get_abs_string_index\u001b[39m(\u001b[38;5;28mself\u001b[39m, idx):\n\u001b[32m    313\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Get the absolute index for the list of modules.\"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m314\u001b[39m     idx = \u001b[43moperator\u001b[49m\u001b[43m.\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m(\u001b[49m\u001b[43midx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    315\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (-\u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m) <= idx < \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m)):\n\u001b[32m    316\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mIndexError\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mindex \u001b[39m\u001b[38;5;132;01m{\u001b[39;00midx\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m is out of range\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mTypeError\u001b[39m: 'str' object cannot be interpreted as an integer"
     ]
    }
   ],
   "source": [
    "attn_blocks['attn_0']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SelfAttention(\n",
       "  (query): Linear(in_features=512, out_features=64, bias=True)\n",
       "  (key): Linear(in_features=512, out_features=64, bias=True)\n",
       "  (value): Linear(in_features=512, out_features=64, bias=True)\n",
       "  (softmax): Softmax(dim=2)\n",
       "  (output): Linear(in_features=64, out_features=512, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attn_blocks.attn_block_0.attn_0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cell-type-classification",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
