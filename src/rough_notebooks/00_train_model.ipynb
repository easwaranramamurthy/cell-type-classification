{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "from tqdm import tqdm\n",
    "import wandb\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from models.transformer import Transformer\n",
    "from datasets.npy_dataset import NpyDataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try balanced loss to account for class imbalance\n",
    "log accuracy\n",
    "balanced accuracy?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<bound method Module.named_modules of Transformer(\n",
      "  (embed): Embedding(27038, 16)\n",
      "  (attn_ops): ModuleList(\n",
      "    (0-3): 4 x SelfAttention(\n",
      "      (query): Linear(in_features=16, out_features=64, bias=True)\n",
      "      (key): Linear(in_features=16, out_features=64, bias=True)\n",
      "      (value): Linear(in_features=16, out_features=64, bias=True)\n",
      "      (softmax): Softmax(dim=-1)\n",
      "      (output): Linear(in_features=64, out_features=16, bias=True)\n",
      "    )\n",
      "  )\n",
      "  (first_layernorms): ModuleList(\n",
      "    (0-3): 4 x LayerNorm((16,), eps=1e-05, elementwise_affine=True)\n",
      "  )\n",
      "  (feed_fwd_layers): ModuleList(\n",
      "    (0-3): 4 x Linear(in_features=16, out_features=16, bias=True)\n",
      "  )\n",
      "  (feed_fwd_activations): ModuleList(\n",
      "    (0-3): 4 x GELU(approximate='none')\n",
      "  )\n",
      "  (second_layernorms): ModuleList(\n",
      "    (0-3): 4 x LayerNorm((16,), eps=1e-05, elementwise_affine=True)\n",
      "  )\n",
      "  (final_mlp): MLP(\n",
      "    (feed_fwd): Linear(in_features=16, out_features=16, bias=True)\n",
      "    (sigmoid): GELU(approximate='none')\n",
      "    (feed_fwd_2): Linear(in_features=16, out_features=47, bias=True)\n",
      "  )\n",
      ")>\n"
     ]
    }
   ],
   "source": [
    "batch_size=8\n",
    "train_dataset = NpyDataset('../data/Xtrain_base.npy', '../data/Ytrain_base.npy')\n",
    "train_dataloader = torch.utils.data.DataLoader(train_dataset,batch_size=batch_size,shuffle=True, num_workers=8)\n",
    "val_dataset = NpyDataset('../data/Xval_base.npy', '../data/Yval_base.npy')\n",
    "val_dataloader = torch.utils.data.DataLoader(val_dataset,batch_size=batch_size,shuffle=False, num_workers=8)\n",
    "\n",
    "vocab = pd.read_csv('../data/vocab.csv', header=None, index_col=0)\n",
    "vocab_size = vocab.shape[0]\n",
    "cat_label_mapping = pd.read_csv('../data/cat_label_mapping.csv', header=None)\n",
    "num_classes = cat_label_mapping.shape[0]\n",
    "context_length = train_dataset.x.shape[1]\n",
    "\n",
    "\n",
    "device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "\n",
    "net = Transformer(num_layers=4,\n",
    "                  vocab_size=vocab_size,\n",
    "                  d_model=16,\n",
    "                  d_q_k_v=16,\n",
    "                  num_heads=4,\n",
    "                  num_classes=num_classes,\n",
    "                  hidden_dim=16)\n",
    "\n",
    "net.to(device=device)\n",
    "optim = torch.optim.AdamW(params = net.parameters())\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "epochs=100\n",
    "\n",
    "print(net.named_modules)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.int64(452239)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sum([param.numel() for param in net.parameters()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(preds, labels):\n",
    "    pred_classes = preds.argmax(dim=1)\n",
    "    correct = (pred_classes == labels).sum().item()\n",
    "    total = labels.size(0)\n",
    "    return correct, total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33measwaran\u001b[0m to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.10"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/Users/easwaranramamurthy/Desktop/git_repos/cell-type-classification/src/wandb/run-20250514_203623-qhfu7er9</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/easwaran/cell_type_classification/runs/qhfu7er9' target=\"_blank\">swift-night-42</a></strong> to <a href='https://wandb.ai/easwaran/cell_type_classification' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/easwaran/cell_type_classification' target=\"_blank\">https://wandb.ai/easwaran/cell_type_classification</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/easwaran/cell_type_classification/runs/qhfu7er9' target=\"_blank\">https://wandb.ai/easwaran/cell_type_classification/runs/qhfu7er9</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 66/7000 [00:23<41:23,  2.79it/s] \n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 7\u001b[39m\n\u001b[32m      5\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mTraining model\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m      6\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m i, (batch_x, batch_y) \u001b[38;5;129;01min\u001b[39;00m tqdm(\u001b[38;5;28menumerate\u001b[39m(train_dataloader), total=\u001b[38;5;28mlen\u001b[39m(train_dataset)):\n\u001b[32m----> \u001b[39m\u001b[32m7\u001b[39m     batch_x = \u001b[43mbatch_x\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m.to(torch.long)\n\u001b[32m      8\u001b[39m     batch_y = batch_y.to(device=device)\n\u001b[32m      9\u001b[39m     optim.zero_grad()\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "wandb.init(project=\"cell_type_classification\",config={})\n",
    "\n",
    "for e in range(epochs):\n",
    "    net.train()\n",
    "    print(\"Training model\")\n",
    "    for i, (batch_x, batch_y) in tqdm(enumerate(train_dataloader), total=len(train_dataset)):\n",
    "        batch_x = batch_x.to(device=device).to(torch.long)\n",
    "        batch_y = batch_y.to(device=device)\n",
    "        optim.zero_grad()\n",
    "        pred = net(batch_x)\n",
    "        loss = loss_fn(pred,batch_y)\n",
    "        loss.backward()\n",
    "        optim.step()\n",
    "        \n",
    "    net.eval()\n",
    "    print(\"Computing validation loss\")\n",
    "    val_loss = 0.0\n",
    "    num_batches = 0\n",
    "    val_correct = 0\n",
    "    val_total = 0\n",
    "    with torch.no_grad():\n",
    "        for batch_x,batch_y in tqdm(val_dataloader, total=len(val_dataset)):\n",
    "            batch_x = batch_x.to(device=device).to(torch.long)\n",
    "            batch_y = batch_y.to(device=device)\n",
    "            pred = net(batch_x)\n",
    "            loss = loss_fn(pred,batch_y).item()\n",
    "\n",
    "            correct, total = accuracy(pred, batch_y)\n",
    "            val_correct += correct\n",
    "            val_total += total\n",
    "\n",
    "            val_loss+=loss\n",
    "            num_batches+=1\n",
    "    \n",
    "    val_loss/=num_batches\n",
    "    val_acc = val_correct / val_total\n",
    "\n",
    "    train_loss = 0.0\n",
    "    num_batches = 0\n",
    "    train_correct = 0\n",
    "    train_total = 0\n",
    "    with torch.no_grad():\n",
    "        for batch_x,batch_y in tqdm(train_dataloader, total=len(train_dataset)):\n",
    "            batch_x = batch_x.to(device=device).to(torch.long)\n",
    "            batch_y = batch_y.to(device=device)\n",
    "            pred = net(batch_x)\n",
    "            loss = loss_fn(pred,batch_y).item()\n",
    "\n",
    "            correct, total = accuracy(pred, batch_y)\n",
    "            train_correct += correct\n",
    "            train_total += total\n",
    "            \n",
    "            train_loss+=loss\n",
    "            num_batches+=1\n",
    "    \n",
    "    train_loss/=num_batches\n",
    "    train_acc = train_correct / train_total\n",
    "\n",
    "    print({\"train_loss\": train_loss, \"train_acc\": train_acc, \"val_loss\": val_loss, \"val_acc\": val_acc})\n",
    "    wandb.log({\"train_loss\": train_loss, \"train_acc\": train_acc, \"val_loss\": val_loss, \"val_acc\": val_acc})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cell-type-classification",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
